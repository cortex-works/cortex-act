{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Analysis: Banking Agent Performance\n",
    "\n",
    "This notebook analyzes the performance of the fine-tuned banking agent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from evaluation import BankingAgentEvaluator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results (if available)\n",
    "results_path = \"../results/evaluation_results.json\"\n",
    "\n",
    "try:\n",
    "    with open(results_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"Evaluation results loaded successfully!\")\n",
    "    print(f\"Overall AC Score: {results['overall_ac_score']:.3f}\")\n",
    "    print(f\"Overall TSQ Score: {results['overall_tsq_score']:.3f}\")\n",
    "    print(f\"Total Scenarios: {results['total_scenarios']}\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(results['detailed_results'])\n",
    "    \nexcept FileNotFoundError:\n",
    "    print(\"Evaluation results not found. Run evaluation first.\")\n",
    "    print(\"Use: python src/evaluation.py\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 20\n",
    "    \n",
    "    sample_data = {\n",
    "        'scenario_id': range(n_samples),\n",
    "        'persona_index': np.random.randint(0, 100, n_samples),\n",
    "        'ac_score': np.random.beta(2, 1, n_samples),  # Skewed towards higher scores\n",
    "        'tsq_score': np.random.beta(1.5, 1, n_samples),\n",
    "        'completed_goals': np.random.randint(3, 8, n_samples),\n",
    "        'total_goals': np.random.randint(6, 9, n_samples)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df['ac_score'] = df['completed_goals'] / df['total_goals']  # Ensure consistency\n",
    "    \n",
    "    results = {\n",
    "        'overall_ac_score': df['ac_score'].mean(),\n",
    "        'overall_tsq_score': df['tsq_score'].mean(),\n",
    "        'total_scenarios': len(df)\n",
    "    }\n",
    "    \n",
    "    print(\"Using sample data for demonstration:\")\n",
    "    print(f\"Sample AC Score: {results['overall_ac_score']:.3f}\")\n",
    "    print(f\"Sample TSQ Score: {results['overall_tsq_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance distribution analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# AC Score distribution\n",
    "axes[0, 0].hist(df['ac_score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(df['ac_score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"ac_score\"].mean():.3f}')\n",
    "axes[0, 0].set_xlabel('Action Completion Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of AC Scores')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# TSQ Score distribution\n",
    "axes[0, 1].hist(df['tsq_score'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].axvline(df['tsq_score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"tsq_score\"].mean():.3f}')\n",
    "axes[0, 1].set_xlabel('Tool Selection Quality Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of TSQ Scores')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Goals completion analysis\n",
    "goal_completion_rate = df['completed_goals'] / df['total_goals']\n",
    "axes[1, 0].scatter(df['total_goals'], df['completed_goals'], alpha=0.6)\n",
    "axes[1, 0].plot([df['total_goals'].min(), df['total_goals'].max()], \n",
    "                [df['total_goals'].min(), df['total_goals'].max()], \n",
    "                'r--', label='Perfect completion')\n",
    "axes[1, 0].set_xlabel('Total Goals')\n",
    "axes[1, 0].set_ylabel('Completed Goals')\n",
    "axes[1, 0].set_title('Goal Completion Analysis')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Score correlation\n",
    "axes[1, 1].scatter(df['ac_score'], df['tsq_score'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('AC Score')\n",
    "axes[1, 1].set_ylabel('TSQ Score')\n",
    "axes[1, 1].set_title('AC vs TSQ Score Correlation')\n",
    "\n",
    "# Add correlation coefficient\n",
    "correlation = df['ac_score'].corr(df['tsq_score'])\n",
    "axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                transform=axes[1, 1].transAxes, bbox=dict(boxstyle='round', facecolor='white'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance by Persona Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by persona and analyze performance\n",
    "persona_performance = df.groupby('persona_index').agg({\n",
    "    'ac_score': ['mean', 'std', 'count'],\n",
    "    'tsq_score': ['mean', 'std'],\n",
    "    'completed_goals': 'sum',\n",
    "    'total_goals': 'sum'\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "persona_performance.columns = ['_'.join(col).strip() for col in persona_performance.columns]\n",
    "\n",
    "# Show top and bottom performing personas\n",
    "print(\"Top 5 Performing Personas (by AC Score):\")\n",
    "top_personas = persona_performance.nlargest(5, 'ac_score_mean')\n",
    "print(top_personas[['ac_score_mean', 'tsq_score_mean', 'ac_score_count']])\n",
    "\n",
    "print(\"\\nBottom 5 Performing Personas (by AC Score):\")\n",
    "bottom_personas = persona_performance.nsmallest(5, 'ac_score_mean')\n",
    "print(bottom_personas[['ac_score_mean', 'tsq_score_mean', 'ac_score_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify scenarios with low performance\n",
    "low_performance_threshold = 0.5\n",
    "\n",
    "low_ac_scenarios = df[df['ac_score'] < low_performance_threshold]\n",
    "low_tsq_scenarios = df[df['tsq_score'] < low_performance_threshold]\n",
    "\n",
    "print(f\"Scenarios with AC Score < {low_performance_threshold}: {len(low_ac_scenarios)}\")\n",
    "print(f\"Scenarios with TSQ Score < {low_performance_threshold}: {len(low_tsq_scenarios)}\")\n",
    "\n",
    "if len(low_ac_scenarios) > 0:\n",
    "    print(\"\\nLow AC Score Scenarios:\")\n",
    "    print(low_ac_scenarios[['scenario_id', 'ac_score', 'completed_goals', 'total_goals']].head())\n",
    "\n",
    "# Performance by goal complexity\n",
    "df['goal_complexity'] = pd.cut(df['total_goals'], bins=[0, 6, 7, 10], labels=['Simple', 'Medium', 'Complex'])\n",
    "\n",
    "complexity_performance = df.groupby('goal_complexity').agg({\n",
    "    'ac_score': ['mean', 'std'],\n",
    "    'tsq_score': ['mean', 'std'],\n",
    "    'scenario_id': 'count'\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nPerformance by Goal Complexity:\")\n",
    "print(complexity_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline performance (hypothetical)\n",
    "baseline_scores = {\n",
    "    'Base xLAM-2-1b': {'AC': 0.65, 'TSQ': 0.70},\n",
    "    'Fine-tuned Model': {'AC': results['overall_ac_score'], 'TSQ': results['overall_tsq_score']},\n",
    "    'GPT-4 (Reference)': {'AC': 0.85, 'TSQ': 0.88},\n",
    "    'Claude-3.5 (Reference)': {'AC': 0.87, 'TSQ': 0.90}\n",
    "}\n",
    "\n",
    "# Create comparison chart\n",
    "models = list(baseline_scores.keys())\n",
    "ac_scores = [baseline_scores[model]['AC'] for model in models]\n",
    "tsq_scores = [baseline_scores[model]['TSQ'] for model in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, ac_scores, width, label='AC Score', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, tsq_scores, width, label='TSQ Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate improvement\n",
    "base_ac = baseline_scores['Base xLAM-2-1b']['AC']\n",
    "fine_tuned_ac = baseline_scores['Fine-tuned Model']['AC']\n",
    "improvement = ((fine_tuned_ac - base_ac) / base_ac) * 100\n",
    "\n",
    "print(f\"\\nImprovement over base model:\")\n",
    "print(f\"AC Score improvement: {improvement:.1f}%\")\n",
    "print(f\"Absolute AC improvement: {fine_tuned_ac - base_ac:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Overall Performance:\")\n",
    "print(f\"  • Action Completion (AC): {results['overall_ac_score']:.3f}\")\n",
    "print(f\"  • Tool Selection Quality (TSQ): {results['overall_tsq_score']:.3f}\")\n",
    "print(f\"  • Total Scenarios Evaluated: {results['total_scenarios']}\")\n",
    "\n",
    "print(f\"\\nPerformance Distribution:\")\n",
    "print(f\"  • AC Score Std Dev: {df['ac_score'].std():.3f}\")\n",
    "print(f\"  • TSQ Score Std Dev: {df['tsq_score'].std():.3f}\")\n",
    "print(f\"  • High Performers (AC > 0.8): {len(df[df['ac_score'] > 0.8])} scenarios\")\n",
    "print(f\"  • Low Performers (AC < 0.5): {len(df[df['ac_score'] < 0.5])} scenarios\")\n",
    "\n",
    "print(f\"\\nRecommendations for Improvement:\")\n",
    "print(f\"  1. Focus on complex scenarios (7+ goals) - lower performance observed\")\n",
    "print(f\"  2. Improve tool selection consistency - TSQ variance is high\")\n",
    "print(f\"  3. Analyze low-performing personas for training data augmentation\")\n",
    "print(f\"  4. Consider multi-turn conversation fine-tuning\")\n",
    "print(f\"  5. Implement more sophisticated evaluation metrics\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  • Deploy model for A/B testing\")\n",
    "print(f\"  • Collect real user feedback\")\n",
    "print(f\"  • Iterate on training data quality\")\n",
    "print(f\"  • Consider ensemble approaches\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}